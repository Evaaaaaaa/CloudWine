{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CloudWine Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the runs folder\n",
    "!rm ./runs/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_config():\n",
    "    config = {\n",
    "      \"model\": \"\",\n",
    "      \"args\": {\n",
    "        \"data_path\": \"./data/raw/winemag-data-130k-v2.csv\",\n",
    "        \"lowercase\": False,\n",
    "        \"remove_punctuation\": False,\n",
    "        \"remove_stopwords\": False,\n",
    "        \"lemmatize\": False,\n",
    "        \"save_model\": False,\n",
    "        \"model_dir\": \"./models/\",\n",
    "        \"save_validation\": True,\n",
    "        \"validation_dir\": \"./runs/\"\n",
    "      }\n",
    "    }\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model(config, model):\n",
    "    config['model'] = model\n",
    "    return config\n",
    "    \n",
    "def set_nlp_args(config, preprocess):\n",
    "    config['args']['lowercase'] = preprocess\n",
    "    config['args']['remove_punctuation'] = preprocess\n",
    "    config['args']['remove_stopwords'] = preprocess\n",
    "    config['args']['lemmatize'] = preprocess\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['tfidf', 'doc2vec', 'bert']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model tfidf\n",
      "[nltk_data] Downloading package stopwords to /Users/elmi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/elmi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/elmi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/elmi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Running model doc2vec\n",
      "[nltk_data] Downloading package stopwords to /Users/elmi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/elmi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/opt/anaconda3/envs/cloudwine/lib/python3.8/site-packages/gensim/models/doc2vec.py:319: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 125, in <module>\n",
      "    main(config)\n",
      "  File \"train.py\", line 24, in main\n",
      "    model.train(corpus)\n",
      "  File \"/Users/elmi/Projects/CloudWine/train/models.py\", line 73, in train\n",
      "    self.model = model\n",
      "  File \"/Users/elmi/Projects/CloudWine/train/models.py\", line 79, in get_vectors\n",
      "    for i in range(1,self.model.corpus_count):\n",
      "  File \"<__array_function__ internals>\", line 5, in vstack\n",
      "  File \"/opt/anaconda3/envs/cloudwine/lib/python3.8/site-packages/numpy/core/shape_base.py\", line 283, in vstack\n",
      "    return _nx.concatenate(arrs, 0)\n",
      "  File \"<__array_function__ internals>\", line 5, in concatenate\n",
      "KeyboardInterrupt\n",
      "[nltk_data] Downloading package stopwords to /Users/elmi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/elmi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/opt/anaconda3/envs/cloudwine/lib/python3.8/site-packages/gensim/models/doc2vec.py:319: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "0/10\n",
      "1/10\n",
      "2/10\n",
      "3/10\n",
      "4/10\n",
      "5/10\n",
      "6/10\n",
      "7/10\n",
      "8/10\n",
      "9/10\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 125, in <module>\n",
      "    main(config)\n",
      "  File \"train.py\", line 24, in main\n",
      "    model.train(corpus)\n",
      "  File \"/Users/elmi/Projects/CloudWine/train/models.py\", line 74, in train\n",
      "    self.text_vectors = self.get_vectors()\n",
      "  File \"/Users/elmi/Projects/CloudWine/train/models.py\", line 80, in get_vectors\n",
      "    docvecs = np.vstack((docvecs, self.model.docvecs[str(i)]))\n",
      "  File \"<__array_function__ internals>\", line 5, in vstack\n",
      "  File \"/opt/anaconda3/envs/cloudwine/lib/python3.8/site-packages/numpy/core/shape_base.py\", line 283, in vstack\n",
      "    return _nx.concatenate(arrs, 0)\n",
      "  File \"<__array_function__ internals>\", line 5, in concatenate\n",
      "KeyboardInterrupt\n",
      "Running model bert\n",
      "[nltk_data] Downloading package stopwords to /Users/elmi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/elmi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "2020-06-11 14:05:10,140 gensim.corpora.dictionary INFO     adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-06-11 14:05:10,140 gensim.corpora.dictionary INFO     built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\n",
      "2020-06-11 14:05:10,454 transformers.file_utils INFO     PyTorch version 1.5.0 available.\n",
      "2020-06-11 14:05:10,552 root         INFO     Load pretrained SentenceTransformer: bert-base-nli-mean-tokens\n",
      "2020-06-11 14:05:10,552 root         INFO     Did not find a '/' or '\\' in the name. Assume to download model from server.\n",
      "2020-06-11 14:05:10,553 root         INFO     Load SentenceTransformer from folder: /Users/elmi/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip\n",
      "2020-06-11 14:05:10,569 transformers.configuration_utils INFO     loading configuration file /Users/elmi/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/config.json\n",
      "2020-06-11 14:05:10,570 transformers.configuration_utils INFO     Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "2020-06-11 14:05:10,570 transformers.modeling_utils INFO     loading weights file /Users/elmi/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/pytorch_model.bin\n",
      "2020-06-11 14:05:12,519 transformers.tokenization_utils INFO     Model name '/Users/elmi/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming '/Users/elmi/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "2020-06-11 14:05:12,520 transformers.tokenization_utils INFO     Didn't find file /Users/elmi/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/tokenizer_config.json. We won't load it.\n",
      "2020-06-11 14:05:12,520 transformers.tokenization_utils INFO     loading file /Users/elmi/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/vocab.txt\n",
      "2020-06-11 14:05:12,520 transformers.tokenization_utils INFO     loading file /Users/elmi/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/added_tokens.json\n",
      "2020-06-11 14:05:12,520 transformers.tokenization_utils INFO     loading file /Users/elmi/.cache/torch/sentence_transformers/public.ukp.informatik.tu-darmstadt.de_reimers_sentence-transformers_v0.2_bert-base-nli-mean-tokens.zip/0_BERT/special_tokens_map.json\n",
      "2020-06-11 14:05:12,520 transformers.tokenization_utils INFO     loading file None\n",
      "2020-06-11 14:05:12,611 root         INFO     Use pytorch device: cpu\n",
      "2020-06-11 14:05:12,613 root         INFO     {'data_path': './data/raw/winemag-data-130k-v2.csv', 'lemmatize': False, 'lowercase': False, 'model_dir': './models/', 'remove_punctuation': False, 'remove_stopwords': False, 'save_model': False, 'save_validation': True, 'validation_dir': './runs/'}\n",
      "2020-06-11 14:05:12,613 root         INFO     Loading data from ./data/raw/winemag-data-130k-v2.csv\n",
      "2020-06-11 14:05:18,665 root         INFO     Training BERT model\n",
      "Encoding bert embeddings\n",
      "Batches:   8%|██▏                        | 1128/13591 [09:40<1:48:57,  1.91it/s]"
     ]
    }
   ],
   "source": [
    "for m in models:\n",
    "    print('Running model ' + m)\n",
    "    config = reset_config()\n",
    "    config = set_model(config, m)\n",
    "    config = set_nlp_args(config, False)\n",
    "\n",
    "    with open('./config.yaml', \"w\") as ff:\n",
    "        yaml.dump(config, ff, default_flow_style=False)\n",
    "\n",
    "    !python3 train.py -y './config.yaml'\n",
    "    \n",
    "    config = reset_config()\n",
    "    config = set_model(config, m)\n",
    "    config = set_nlp_args(config, True)\n",
    "\n",
    "    with open('./config.yaml', \"w\") as ff:\n",
    "        yaml.dump(config, ff, default_flow_style=False)\n",
    "\n",
    "    !python3 train.py -y './config.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "files = [f for f in listdir('./runs') if isfile(join('./runs', f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = files[0]\n",
    "with open(f, \"r\") as file:\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "    print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudwine",
   "language": "python",
   "name": "cloudwine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
